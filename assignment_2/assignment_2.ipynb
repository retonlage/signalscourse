{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 2 EEG Data Analysis\n",
    "\n",
    "*This assignment is an exploration into a dataset of EEG data on patients. The best description given by the authors of the original data is \"Resting state EEG with closed eyes and open eyes in females from 60 to 80 years old\", found here:\n",
    "'https://openneuro.org/datasets/ds005420/versions/1.0.0'. The data contains EEG measurements using 20 electrodes where the subjects where asked to keep their eyes open (oa) and then closed (oc) for around 4-5 minutes. For the purposes of this exercise we have taken a subset of 8 subjects using only 200 seconds of each of oc and oc to align the sizes.*\n",
    "\n",
    "*The main aim is to give you an application of the Discrete Fourier Transform on real world data. As this step can be done in a single function a lot of the time spent on this exercise will be on visually inspecting the data and trying different classifiers on the data you have produced. If you are lucky enough to draw this assignment for your exam you should be prepared to answer mainly theoretical questions about the Fourier Transform as covered in the exercises and lectures from week 5&6.*\n",
    "\n",
    "Apart from the usual libraries you have been using in this course, you will have to install mne via pip or conda."
   ],
   "id": "44f4f4c935c06e5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.signal as signal\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_data = \"egg_data_assignment_2\"\n",
    "data_oa_original = []\n",
    "data_oc_original = []\n",
    "sample_rate = 50\n",
    "sample_rate_original = 250\n",
    "\n",
    "for folder in tqdm(os.listdir(path_data), desc=\"Loading data\"):\n",
    "    for filename in os.listdir(join(path_data, folder)):\n",
    "        df = pd.read_csv(join(path_data, folder, filename), sep=\",\", index_col=0)\n",
    "        if \"oa\" in filename:\n",
    "            data_oa_original.append(df.values)\n",
    "        else:\n",
    "            data_oc_original.append(df.values)"
   ],
   "id": "72eb7f9b0506a7d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(F\"Loaded data from {len(data_oa_original)} patients\")"
   ],
   "id": "e07fa0fa0fdca4b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1\n",
    "**1. Let's take a look at the data by investigating the shape of it. What is the maximum frequency we can represent given the length?**\n",
    "\n",
    "*Hint: Remember to factor in the sample rate*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c2e72291a368a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for idx, (d_oc, d_oa) in enumerate(zip(data_oc_original, data_oa_original)):\n",
    "    print(f\"Shape of OC: {d_oc.shape}, Shape of OA: {d_oa.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d17947dc4e958b0f",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Now we want to center all the data around the same value (0), in eeg data analysis terms this is called referencing, and down sample in order to have a manageable size that a normal pc with 16 Gb of RAM can manipulate. Here we will apply a simple average referencing*\n",
    "\n",
    "**2. Implement the following function:**\n"
   ],
   "id": "624394fe2adc8827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_reference_and_downsample(eeg_data, ref_channels, sample_rate=50):\n",
    "    \"\"\"Take the mean of the reference channels and subtract that from the data\"\"\"\n",
    "    eeg_masked = eeg_data[ref_channels]\n",
    "    eeg_down_sampled = signal.resample(eeg_masked, eeg_masked.shape[1] // (sample_rate_original // sample_rate), axis=1)\n",
    "    \n",
    "    ref_signal = np.mean(?, axis=?)\n",
    "    re_referenced_data = ?\n",
    "    return re_referenced_data"
   ],
   "id": "c6b5c739d7cc1eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*We will reference and down sample each patient's data individually:*"
   ],
   "id": "dcd5cfbece3a0749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_oc = []\n",
    "data_oa = []\n",
    "for i in range(len(data_oa_original)):\n",
    "    data_oc.append(set_reference_and_downsample(data_oc_original[i], np.arange(20), sample_rate=sample_rate))\n",
    "    data_oa.append(set_reference_and_downsample(data_oa_original[i], np.arange(20), sample_rate=sample_rate))"
   ],
   "id": "8c499f3581366b2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for idx, (d_oc, d_oa) in enumerate(zip(data_oc, data_oa)):\n",
    "    print(f\"Shape of OC: {d_oc.shape}, Shape of OA: {d_oa.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e2185d106c6820",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. Using the following plotting function, have a look at the data and comment on the quality of it:**\n"
   ],
   "id": "963e6a354bbb9740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_eeg_mne_style(eeg_data, ch_names, sampling_rate):\n",
    "    \"\"\"\n",
    "    Plot EEG data in an MNE-style plot for each channel.\n",
    "\n",
    "    Parameters:\n",
    "    eeg_data : np.ndarray\n",
    "        The EEG data with shape (n_channels, n_points).\n",
    "    ch_names : list of str\n",
    "        List of channel names (must be of length n_channels).\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the EEG data (in Hz).\n",
    "    \"\"\"    \n",
    "    # Create MNE info structure with channel names and sampling rate\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sampling_rate, ch_types='eeg')\n",
    "    \n",
    "    # Create MNE RawArray object with the data\n",
    "    raw = mne.io.RawArray(eeg_data, info)\n",
    "    \n",
    "    # Plot the EEG data with MNE's built-in plot function\n",
    "    raw.plot(scalings='auto', show=True, block=True)\n",
    "    plt.show()"
   ],
   "id": "f93c0c344b054dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "channel_names = np.arange(1,21,1).astype(str).tolist()\n",
    "plot_eeg_mne_style(data_oa[4], ch_names=channel_names, sampling_rate=sample_rate)"
   ],
   "id": "a6ba5f9c31c30d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2 Discrete Fourier Transform\n",
    "Given the sample rate and duration of the EEG signals we have just loaded, complete this exercise using the signal from the first channel of the first OC patient. \n",
    "\n",
    "**1. Using the formula for fourier coefficients: $c_k = \\sum_{n = 0}^{N-1} x(n) e^{-i 2\\pi \\frac{kn}{N}}$, calculate the fourier coefficient for the signal corresponding to $k = 25$**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a0ef57a85dd0527"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N = ?\n",
    "fs = ?\n",
    "k = ?\n",
    "coeff_k = 0\n",
    "for n, x_n in enumerate(data_oc[0][0]):\n",
    "    coeff_k += ?\n",
    "print(f\"Fourier coefficient {k} = {coeff_k:.3}\")\n",
    "print(f\"Absolute value = {np.abs(coeff_k):.3}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c71b6da11121366",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Calculate the frequency that the fourier coefficient for $k = 25$ corresponds to**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**3. Calculate the amplitude of the frequency that corresponds to the fourier coefficient for $k = 1$**\n",
    "\n",
    "$\\dots$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcfc4d5bce9db600"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3 Frequency Domain and Filtering\n",
    "\n",
    "*In this exercise you will take a time domain filter, plot its frequency response and use it to filter the data. Let's create the orthogonal basis matrix, but here in a vectorised function because otherwise it'd take too long.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed7edccfb37cc07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_basis(N, fs):\n",
    "    # Ensure N is odd\n",
    "    assert N % 2 == 1\n",
    "\n",
    "    ns = np.arange(N)\n",
    "    n_pairs = int((N - 1) / 2)\n",
    "\n",
    "    freqs = np.arange(int(0.5 * N) + 1) * fs / N\n",
    "\n",
    "    # Create cosine and sine terms for all frequencies at once\n",
    "    ks = np.arange(1, n_pairs + 1)\n",
    "    angles = 2 * np.pi * np.outer(ns, ks) / N\n",
    "\n",
    "    cosines = np.sqrt(2) * np.cos(angles)\n",
    "    sines = np.sqrt(2) * np.sin(angles)\n",
    "\n",
    "    # Create basis matrix with first column of ones, then interleave cosines and sines\n",
    "    V = np.ones((N, N))\n",
    "    V[:, 1::2] = cosines  # Fill odd-indexed columns with cosines\n",
    "    V[:, 2::2] = sines    # Fill even-indexed columns with sines\n",
    "\n",
    "    # Normalize by sqrt(N)\n",
    "    V /= np.sqrt(N)\n",
    "\n",
    "    return freqs, V\n",
    "\n",
    "def compute_spectrum(y, V, scale=True):\n",
    "    z = V.T@y\n",
    "\n",
    "    z_cos = z[2::2] \n",
    "    z_sin = z[1::2]\n",
    "    amp_spectrum = np.sqrt(z_cos**2 + z_sin**2)\n",
    "    amp_spectrum = np.hstack((np.abs(z[0]), amp_spectrum))\n",
    "    \n",
    "    if scale:\n",
    "        scales = np.ones(len(amp_spectrum))\n",
    "        scales[0] = 1\n",
    "        scales[1:] = np.sqrt(2)\n",
    "\n",
    "\n",
    "        return amp_spectrum/np.sqrt(len(y))*scales\n",
    "    else:\n",
    "        return amp_spectrum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc87c5cf91274bb5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N_odd = N-1\n",
    "# prepare basis matrix V\n",
    "freqs, V = create_basis(N-1, sample_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c379bb86de560d6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Below you are given a time domain filter, plot it and comment on its shape. How do you think it does its filtering?**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5840c5f60b785bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "h = np.array([0.00206512, -0.00246738, -0.00061028, -0.00412665, -0.00860358, 0.00576032,\n",
    "                 -0.00815993, 0.02454267, 0.0238489, 0.00708215, 0.05402169, -0.07304967,\n",
    "                 -0.01784375, -0.11967235, -0.28945082, 0.40677716, 0.40677716, -0.28945082,\n",
    "                 -0.11967235, -0.01784375, -0.07304967, 0.05402169, 0.00708215, 0.0238489,\n",
    "                 0.02454267, -0.00815993, 0.00576032, -0.00860358, -0.00412665, -0.00061028,\n",
    "                 -0.00246738, 0.00206512])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21d552fdeb471b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(h)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "928175b8bfcc6cea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.Using the function that computes the spectrum of a signal to look at the frequency response of the filter? How do you make the dimensions match between the filter and the orthogonal basis matrix?**\n",
    "*Hint: use padding*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e0f258e5c056f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "htilde = ?\n",
    "htilde[?] = ?\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(freqs, compute_spectrum(?))\n",
    "plt.xlabel('Frequency ')\n",
    "plt.title('Frequency response of filter h')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34527f2749b08f3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*$\\star$ In the following cell we the time domain filter, this is outside the scope of the course, but a very interesting topic to look into if you have the time. In addition there is a simpler function for calculating the frequency response:)*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f628c13fcfb0e587"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_bandpass_kernel(lowcut, highcut, fs, numtaps=32):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "\n",
    "    # Design a bandpass FIR filter\n",
    "    kernel = signal.firwin(numtaps, [low, high], pass_zero=False)\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "def frequency_response(filter_kernel, fs, padding=1000):\n",
    "    # Compute the FFT of the filter kernel (rfft for real inputs)\n",
    "    freq_response = np.abs(np.fft.rfft(filter_kernel, padding)) \n",
    "    \n",
    "    # Compute the corresponding frequency axis\n",
    "    freqs = np.abs(np.fft.rfftfreq(padding, 1/fs))  \n",
    "\n",
    "    return freqs, freq_response\n",
    "print(create_bandpass_kernel(5, 20, fs, numtaps=32))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2cf965945bf4182",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*And here is a function to plot the frequency response of the filter using the above function:*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6a75553573ad8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "freqs_h, freq_response_h = frequency_response(h, sample_rate)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(freqs_h, freq_response_h)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=12)\n",
    "plt.ylabel('Magnitude (dB)', fontsize=12)\n",
    "plt.title('Frequency Response of Filter', fontsize=14)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65d6e71dc0b7da8b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Implement a function that filters a patients eeg signal in the time domain:**\n",
    "- Compare the data before and after filtering\n",
    "- Try different thresholds like (1 to 3) and (40 to 70) and comment on the effects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6f4d824e86f7fce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def bandpass_filter(signal_data, kernel):\n",
    "    output = []\n",
    "    for channel in signal_data:\n",
    "        filtered_signal = np.convolve(?, mode='same')  # Convolve with the signal\n",
    "        output.append(filtered_signal)\n",
    "    output = np.array(output)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91789f9c8c641eec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filtered_channels = bandpass_filter(data_oc[0], h)\n",
    "plot_eeg_mne_style(data_oc[0], ch_names=channel_names, sampling_rate=sample_rate)\n",
    "plot_eeg_mne_style(np.array(filtered_channels), ch_names=channel_names, sampling_rate=sample_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdd778f3fe60150",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*$\\star$ The function below does what we have just implemented in two separate functions, feel free to use it:)*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47450f1ea20ea8be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def bandpass_filter_butter(data, lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    \n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    \n",
    "    return filtered_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "825f14a8e11dc820",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**3. Complete the following function which takes a signal and computes its power spectrum and removes frequencies outside the min and max in order to remove redundant information after filtering**\n",
    "- Discuss the difference between computing the power spectrum of a signal and the frequency response of a filter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e69cbb08d7b7d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_power_spectrum(signal, sampling_rate, min_freq=None, max_freq=None):\n",
    "    \"\"\"\n",
    "    Computes the power spectrum for each channel in the signal.\n",
    "\n",
    "    Parameters:\n",
    "    signal : np.ndarray\n",
    "        Input signal with shape (n_channels, n_points)\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the signal (in Hz).\n",
    "\n",
    "    Returns:\n",
    "    freqs : np.ndarray\n",
    "        The frequencies corresponding to the power spectrum.\n",
    "    power_spectrum : np.ndarray\n",
    "        Power spectrum of each channel with shape (n_channels, n_points//2).\n",
    "    \"\"\"\n",
    "    n_channels, n_points = signal.shape\n",
    "    \n",
    "    # Perform the FFT for each channel\n",
    "    fft_vals = np.fft.rfft(signal, axis=1)\n",
    "    \n",
    "    # Compute the power spectrum (squared magnitude of the FFT)\n",
    "    power_spectrum = np.abs(fft_vals) ** 2\n",
    "    \n",
    "    # Compute the corresponding frequencies\n",
    "    freqs = np.fft.rfftfreq(n_points, 1 / sampling_rate)\n",
    "    \n",
    "    if min_freq is not None:\n",
    "        power_spectrum = power_spectrum[?]\n",
    "        freqs = freqs[?]\n",
    "\n",
    "    if max_freq is not None:\n",
    "        power_spectrum = power_spectrum[?]\n",
    "        freqs = freqs[?]\n",
    "        \n",
    "    return freqs, power_spectrum"
   ],
   "id": "8a510404cd8586bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*This next function is just for plotting*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7342b0b6d4368054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_power_spectrum_overlay(freqs, power_spectrum, ch_names, show=False, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the power spectrum for all channels on the same plot.\n",
    "\n",
    "    Parameters:\n",
    "    eeg_data : np.ndarray\n",
    "        The EEG data with shape (n_channels, n_points).\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the EEG data (in Hz).\n",
    "    ch_names : list of str\n",
    "        List of channel names.\n",
    "    \"\"\"\n",
    "    colors = cm.viridis(np.linspace(0, 1, len(ch_names)))\n",
    "\n",
    "    for i, ch_name in enumerate(ch_names):\n",
    "        plt.plot(freqs, power_spectrum[i], color=colors[i], label=ch_name)\n",
    "\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.title('Power Spectrum for EEG Channels' if not title else title)\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ],
   "id": "1aa142f3108f4b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*We have been advised by Ivana (Human Cognition lecturer and EEG researcher), that we should take a look at the frequencies between 5 and 20 Hz in order to get a distinction between OC and OA. Here we plot all the power spectra from each channel on top of one each other. Feel free to investigate other indexes.*"
   ],
   "id": "1cc2b359a450d951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "patient_indexes = [0, 3, 5]\n",
    "min_freq=1\n",
    "max_freq=20\n",
    "power_spectra_oc = []\n",
    "power_spectra_oa = []\n",
    "y_max = 3e-4\n",
    "\n",
    "for segment in patient_indexes:\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    data_oc_time_filtered = bandpass_filter(data_oc[segment], h)\n",
    "    data_oa_time_filtered = bandpass_filter(data_oa[segment], h)\n",
    "    # Compute power spectra for both conditions\n",
    "    freqs_oc, power_spectrum_oc = compute_power_spectrum(data_oc_time_filtered, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    freqs_oa, power_spectrum_oa = compute_power_spectrum(data_oa_time_filtered, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    power_spectra_oc.append((freqs_oc, power_spectrum_oc))\n",
    "    power_spectra_oa.append((freqs_oa, power_spectrum_oa))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, power_spectrum_oc, channel_names, title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.ylim([0, y_max])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, power_spectrum_oa, channel_names, title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim([0, y_max])\n",
    "    plt.show()"
   ],
   "id": "866e991737b1e773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Using the loop above take a look at the spectrograms for eyes open and eyes closed.** \n",
    "- Is there a noticeable difference between the two? How could we use this difference for classification?\n",
    "- Discuss whether the conditions of our fft are sufficient to avoid spectral leakage\n",
    " \n",
    " "
   ],
   "id": "2cc6b501ca8a5201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. The channels at index 17 and 18 are known to be correlated to have open vs closed eyes, plot these:**"
   ],
   "id": "b2e9d81173f615e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optical_channel_slice = slice(?)    # Look it up if in doubt as to how a slice works\n",
    "data_oc_optical_channels = [arr[optical_channel_slice] for arr in data_oc]\n",
    "data_oa_optical_channels = [arr[optical_channel_slice] for arr in data_oa]"
   ],
   "id": "a0dee92ef5b2699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "patient_indexes = [0, 3, 5]\n",
    "min_freq=5\n",
    "max_freq=20\n",
    "for segment in patient_indexes:\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    data_oc_time_filtered = bandpass_filter(data_oc_optical_channels[segment], h)\n",
    "    data_oa_time_filtered = bandpass_filter(data_oa_optical_channels[segment], h)\n",
    "    # Compute power spectra for both conditions\n",
    "    freqs_oc, power_spectrum_oc = compute_power_spectrum(data_oc_time_filtered, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    freqs_oa, power_spectrum_oa = compute_power_spectrum(data_oa_time_filtered, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, power_spectrum_oc, channel_names[optical_channel_slice], title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.ylim([0, y_max])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, power_spectrum_oa, channel_names[optical_channel_slice], title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim([0, y_max])\n",
    "    plt.show()"
   ],
   "id": "204d7550f9fbb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Now get the power spectra from the range that seems to be the most suited for classifying the difference between OC and OA based on the visual inspection you did above:**"
   ],
   "id": "24c7417b4d7d6136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_freq = ?\n",
    "max_freq = ?\n",
    "\n",
    "power_spectra_oc = [compute_power_spectrum(arr, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq) for arr in data_oc_optical_channels]\n",
    "power_spectra_oa = [compute_power_spectrum(arr, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq) for arr\n",
    "                    in data_oa_optical_channels]"
   ],
   "id": "8c21a3a712fcdbda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Let's plot this smaller spectrum*"
   ],
   "id": "40d377e2e3f31458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for idx in patient_indexes:\n",
    "    freqs_oc, spectrum_oc = power_spectra_oc[idx]\n",
    "    freqs_oa, spectrum_oa = power_spectra_oa[idx]\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, spectrum_oc, ch_names=[17, 18,], title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.ylim([0, y_max])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, spectrum_oa, ch_names=[17, 18,], title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim([0, y_max])\n",
    "    plt.show()"
   ],
   "id": "66ee93e5c34964f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. Now let's compute the energies in this range, typically referred to as the alpha band, and plot them**\n",
    "\n",
    "\n",
    "*Hint: recall that the energy of a signal is given by $E = \\sum_{n=0}^N f(n)^2$*"
   ],
   "id": "f56350331394ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "alphas_oc = []\n",
    "alphas_oa = []\n",
    "for (freqs_oc, spectrum_oc), (freqs_oa, spectrum_oa) in zip(power_spectra_oc, power_spectra_oa):\n",
    "    alpha_oc = ?\n",
    "    alpha_oa = ?\n",
    "    \n",
    "    alphas_oc.append(alpha_oc)\n",
    "    alphas_oa.append(alpha_oa)\n",
    "alphas_oc = np.array(alphas_oc)\n",
    "alphas_oa = np.array(alphas_oa)"
   ],
   "id": "143cb7580a1d651b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_alphas(alphas_oc, alphas_oa, show=False, s = 10):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(range(len(alphas_oc)), alphas_oc, label='OC', color='blue', s=s) \n",
    "    plt.scatter(range(len(alphas_oa)), alphas_oa, label='OA', color='orange', s=s)\n",
    "    # The following line makes a line which you can change to illustrate a decision boundary:\n",
    "    plt.title('OA and OC energy of alpha frequencies compared')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ],
   "id": "540eaab0fef02a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_alphas(alphas_oc, alphas_oa, show=True)"
   ],
   "id": "3172674ee0f16a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6. Now that you have processed the data using the dft and inspected the energies of the two different cases, discuss how well you believe this data could be used to create a classifier that classifies whether a person has their eyes open or closed:**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28e5138785bee5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4 Classification\n",
    "\n",
    "*This exercise is here to ties what you learned in weeks 1-3 together with signals processing.\n",
    " Firstly we will use a Naive Bayes classifier and then we will implement a CNN to do classification on the EEG data. Here you can spend as much time and effort as you wish refining the classification or coming up with other techniques, but be warned EEG data is notoriously noisy.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "989b036a88fe269b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Implement a Naive Bayes classifier using the entire dataset to plot the posterior distribution.**\n",
    "\n",
    "*A naive Bayes classifier assumes that the features used for classification are normally distributed according to their empirical mean and variance. The steps for classification are as follows:*\n",
    "1. Find the mean and variance of the features of each class for the calculation of the likelihood $P(\\alpha|C_x) = \\mathcal{N}(\\mu_{\\alpha_x}, \\sigma_{\\alpha_x}^2)$\n",
    "    - In our case the features are just the alpha energy\n",
    "2. Define a prior distribution for each class given the features \n",
    "    - Here we use a uniform prior representing the proportion of each class in the dataset, why do you think this makes sense? $Prior(C) = 0.5$\n",
    "3. Given a new value, calculate the posterior probability of belonging to OC (class 0) using Bayes theorem:\n",
    "        $$\n",
    "        P(C_0|\\alpha) = \\frac{P(\\alpha|C_0) \\cdot Prior(C_0)}{P(\\alpha|C_0) \\cdot Prior(C_0) + P(\\alpha|C_1) \\cdot Prior(C_1)}\n",
    "        $$\n",
    "        $$ \n",
    "        P(C_1|\\alpha) = 1- P(C_0|\\alpha)\n",
    "        $$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299ab97f19e968b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*1 Compute the means and variances*:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289d64b1baabe432"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m0, v0 = ?\n",
    "m1, v1 = ?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74bf4c4dcf4f18a1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*2 Define the priors along with a helper function for calculating the pdf of a normal distribution:*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baa357cfa75616af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def normal_pdf(x, m, v):\n",
    "    return np.exp(-(x - m) ** 2 / (2 * v)) / np.sqrt(2 * np.pi * v)\n",
    "\n",
    "def naive_bayes_prior_0(alpha_energy):\n",
    "    return ?\n",
    "\n",
    "def naive_bayes_prior_1(alpha_energy):\n",
    "    return ?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e7834f3df2a63bf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*3 We will define a function to compute the posterior distribution and one to use that for classification:*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3df468bc39dcbc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_posterior(alpha_energy, m0, v0, m1, v1):\n",
    "    likelihood_0 = ?\n",
    "    likelihood_1 = ?\n",
    "    \n",
    "    prior_0 = ?\n",
    "    prior_1 = ?\n",
    "    \n",
    "    return ?\n",
    "\n",
    "def classify_naive_bayes(alpha_energy, m0, v0, m1, v1):\n",
    "    return ? >= 0.5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97755cbe9c042ef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Now we plot the posterior probability of a given observation being OC according to our classifier. Make sure you understand what this plot represents, do ask if in doubt.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b6635a2e1319f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "xs = np.linspace(1e-10, 3e-5, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(xs, compute_posterior(xs, m0, v0, m1, v1))\n",
    "plt.xlabel('Alpha energy')\n",
    "plt.title('Posterior probability of eyes closed (OC)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96ac18ee5fae2159",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The following is a helper function for the cross-validation loop we are about to do:* "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdc6a65f4244e7d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_validation_split(data_oc, data_oa, hold_out_val, i, device, as_arrays=False):\n",
    "    \"\"\"\n",
    "    Performs cross-validation split for one of time, frequency, or alpha data.\n",
    "    \n",
    "    Args:\n",
    "        data_oc (list or array): Optical channels (oc) data.\n",
    "        data_oa (list or array): Optical amplifiers (oa) data.\n",
    "        hold_out_val (int): Number of elements to hold out in each fold.\n",
    "        i (int): Current fold index.\n",
    "    \n",
    "    Returns:\n",
    "        train_current (list or array): Training set for current fold.\n",
    "        val_current (list or array): Validation set for current fold.\n",
    "    \"\"\"    \n",
    "    # Define slices for validation and training sets\n",
    "    slice_val = slice(i * hold_out_val, (i + 1) * hold_out_val)\n",
    "    before_slice = slice(0, i * hold_out_val)\n",
    "    after_slice = slice((i + 1) * hold_out_val, None)\n",
    "    # Validation sets\n",
    "    val_current_oc = data_oc[slice_val]\n",
    "    val_current_oa = data_oa[slice_val]\n",
    "    \n",
    "    # Training sets (excluding validation slices)\n",
    "    train_current_oc = data_oc[before_slice] + data_oc[after_slice]\n",
    "    train_current_oa = data_oa[before_slice] + data_oa[after_slice]\n",
    "    \n",
    "    # Combine oc and oa for the current fold\n",
    "    val_current = val_current_oc + val_current_oa\n",
    "    train_current = train_current_oc + train_current_oa\n",
    "    \n",
    "    val_targets = [0 for _ in val_current_oc] + [1 for _ in val_current_oa]\n",
    "    train_targets = [0 for _ in train_current_oc] + [1 for _ in train_current_oa]\n",
    "    if as_arrays:\n",
    "        return train_current_oc, train_current_oa, val_current_oc, val_current_oa\n",
    "    else:\n",
    "        train_set = SimpleDataset(train_current, train_targets, device)\n",
    "        val_set = SimpleDataset(val_current, val_targets, device)\n",
    "        \n",
    "        return train_set, val_set\n",
    "        "
   ],
   "id": "f43f4287afc3f932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Define the parameters of the CNN**\n",
    "\n",
    "*If you want to use the LazyLinear layer you need the newest version of Pytorch*"
   ],
   "id": "f9019154629579d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.lr = lr\n",
    "        self.define_network()\n",
    "    \n",
    "    def define_network(self):\n",
    "        # Define layers as a torch.nn.Sequential object\n",
    "        # This is pretty nice, since we can just go layers(input) to get output\n",
    "        # Rather than having a bunch of functions in the forward function\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=?, kernel_size=3, padding=1), # dim = in\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=?),    # Automatically infers the input dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=?, out_features=?),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=?, out_features=self.num_classes)\n",
    "        ).to(device)\n",
    "                \n",
    "        # Loss function and optimizer, as you know, Adam is the meta\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def train(self, train_dataloader, epochs=1, val_dataloader=None, run_filepath=\"\"):\n",
    "        \n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for epoch in tqdm(range(epochs), desc=f'Training'):\n",
    "            \n",
    "            path_epoch = join(run_filepath, f\"{epoch}.pth\")\n",
    "            os.makedirs(os.path.dirname(path_epoch), exist_ok=True)\n",
    "            if os.path.exists(path_epoch):\n",
    "                self.load_model(path_epoch)\n",
    "                print(f\"Loaded model from {path_epoch}. Skipping epoch.\")\n",
    "                \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Not actually used for training, just for keeping track of accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            \n",
    "            self.save_model(path_epoch)\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "            print(f\"Epoch {epoch} training accuracy: {epoch_acc / len(train_dataloader.dataset)}\")\n",
    "\n",
    "            # If we have val dataloader, we can evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                acc = self.eval(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval(self, test_dataloader):\n",
    "        \n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            # Get predictions\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            # Remember, outs are probabilities (so there's 10 for each input)\n",
    "            # The classification the network wants to assign, must therefore be the probability with the larget value\n",
    "            # We find that using argmax (dim=1, because dim=0 would be across batch dimension)\n",
    "            classifications = torch.argmax(logits, dim=1)\n",
    "            total_acc += (classifications == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': self.optim.state_dict()\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    # Load model method\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")"
   ],
   "id": "dd3df8f0a6e59d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. Define the parameters of a Feed Forward Neural Network**"
   ],
   "id": "9e8bba7a4f6fbbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FFNN(CNN):\n",
    "    def __init__(self, num_classes, in_channels=1, lr=0.001):\n",
    "        super().__init__(num_classes, in_channels, lr)\n",
    "        \n",
    "    def define_network(self):\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=?),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=?, out_features=?),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=?, out_features=self.num_classes)\n",
    "        ).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=self.lr)"
   ],
   "id": "40f3dafa5ae3b2f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*We also need a boilerplate torch dataset definition in order to use our data:*"
   ],
   "id": "abc38307a941714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, time_data, targets, device, dtype_data=torch.float32, dtype_targets=torch.long):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            time_data (list or array): Training time data.\n",
    "            targets (list or array): Corresponding target values (labels).\n",
    "        \"\"\"\n",
    "        self.data = time_data\n",
    "        self.targets = targets\n",
    "        self.device = device\n",
    "        self.dtype_data = dtype_data\n",
    "        self.dtype_targets = dtype_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        assert len(self.data) == len(self.targets)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample (time data and corresponding target)\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        sample = torch.tensor(sample).to(dtype=self.dtype_data).to(device)\n",
    "        target = torch.tensor(target).to(dtype=self.dtype_targets).to(device)\n",
    "        if sample.ndim == 2:\n",
    "            sample = sample.unsqueeze(0)\n",
    "        \n",
    "        # Optionally convert to torch tensors if needed\n",
    "        return sample, target"
   ],
   "id": "6a58353ede3f164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Now use the models you have defined above in cross-validation loops, where we will lazily omit the testing part and rely solely on validation, that each use the EEG data in different ways. Here we have set up the following 4 loops, but feel free to experiment with other configurations (Feel free to try other combinations):**\n",
    "\n",
    "- Naive Bayes on alpha energy\n",
    "- CNN on frequency domain\n",
    "- CNN on Time domain\n",
    "- FFNN on frequency domain\n",
    "\n",
    "*The pytorch models will be saved in a folder named 'models' and can be loaded from there. The test loop automatically loads a model if it has the same name as the one it is about to train thus skipping the epoch.*"
   ],
   "id": "5a9afc040f8eacab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "hold_out_val = 1\n",
    "hold_out_test = 0\n",
    "\n",
    "# Assuming the oc and oa data are already concatenated, split them back to separate.\n",
    "train_set_time_oc = data_oc_optical_channels[hold_out_test:]\n",
    "train_set_time_oa = data_oa_optical_channels[hold_out_test:]\n",
    "\n",
    "train_set_frequency_oc = [el[1] for el in power_spectra_oc[hold_out_test:]]\n",
    "train_set_frequency_oa = [el[1] for el in power_spectra_oa[hold_out_test:]]\n",
    "\n",
    "train_set_alpha_oc = alphas_oc[hold_out_test:]\n",
    "train_set_alpha_oa = alphas_oa[hold_out_test:]\n",
    "\n",
    "k_folds = len(train_set_alpha_oc) // hold_out_val"
   ],
   "id": "42a941d77d0a3e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-naive-bayes\")\n",
    "\n",
    "val_accuracies_naive_bayes = []\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_oc, train_oa, val_oc, val_oa = cross_validation_split(alphas_oc.tolist(), alphas_oa.tolist(), hold_out_val, i, device=device, as_arrays=True)\n",
    "    \n",
    "    m0, v0 = np.mean(train_oc), np.var(train_oc)\n",
    "    m1, v1 = np.mean(train_oa), np.var(train_oa)\n",
    "    acc_oc = sum([classify_naive_bayes(el, m0, v0, m1, v1) == 1 for el in val_oc])\n",
    "    acc_oa = sum([classify_naive_bayes(el, m0, v0, m1, v1) == 0 for el in val_oa])\n",
    "    acc = (acc_oc + acc_oa) / (len(val_oc) + len(val_oa))\n",
    "    \n",
    "    val_accuracies_naive_bayes.append(acc)"
   ],
   "id": "ff10f11080f4fb86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-time-CNN\")\n",
    "val_accuracies_cnn_time = []\n",
    "epochs_CNN_time = 1\n",
    "# Cross-validation loop\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_time_oc, train_set_time_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    CNN_time = CNN(num_classes=2)\n",
    "    CNN_time.train(DataLoader(train_set_current), epochs=epochs_CNN_time, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_time = CNN_time.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_cnn_time.append(val_cnn_time)"
   ],
   "id": "b857ba379b98b879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-CNN\")\n",
    "val_accuracies_cnn_freq = []\n",
    "epochs_CNN_freq = 10\n",
    "\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_frequency_oc, train_set_frequency_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    CNN_freq = CNN(num_classes=2)\n",
    "    CNN_freq.train(DataLoader(train_set_current), epochs=epochs_CNN_freq, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_freq = CNN_freq.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_cnn_freq.append(val_cnn_freq)"
   ],
   "id": "83bac708ea3d9e68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-FFNN\")\n",
    "epochs_FFNN = 2\n",
    "val_accuracies_fnn_freq = []\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_frequency_oc, train_set_frequency_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    FFNN_freq = FFNN(num_classes=2)\n",
    "    FFNN_freq.train(DataLoader(train_set_current), epochs=epochs_FFNN, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_freq = FFNN_freq.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_fnn_freq.append(val_cnn_freq)"
   ],
   "id": "8b2e627fd7b215e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate means\n",
    "# Small value to prevent division by zero\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Calculate means\n",
    "mean_cnn_time = [sum(val_accuracies_cnn_time) / (len(val_accuracies_cnn_time) + epsilon)] * len(val_accuracies_cnn_time)\n",
    "mean_cnn_freq = [sum(val_accuracies_cnn_freq) / (len(val_accuracies_cnn_freq) + epsilon)] * len(val_accuracies_cnn_freq)\n",
    "mean_fnn_freq = [sum(val_accuracies_fnn_freq) / (len(val_accuracies_fnn_freq) + epsilon)] * len(val_accuracies_fnn_freq)\n",
    "mean_threshold = [sum(val_accuracies_naive_bayes) / (len(val_accuracies_naive_bayes) + epsilon)] * len(val_accuracies_naive_bayes)\n",
    "\n",
    "# Plotting\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "cnn_time_color = 'blue'  # Example color for CNN Time\n",
    "cnn_freq_color = 'orange'  # Example color for CNN Freq\n",
    "fnn_freq_color = 'green'  # Example color for FNN Freq\n",
    "threshold_color = 'red'  # Example color for Threshold\n",
    "\n",
    "plt.plot(val_accuracies_cnn_time, label='CNN Time', marker='o', color=cnn_time_color)\n",
    "plt.plot(val_accuracies_cnn_freq, label='CNN Freq', marker='s', color=cnn_freq_color)\n",
    "plt.plot(val_accuracies_fnn_freq, label='FNN Freq', marker='^', color=fnn_freq_color)\n",
    "plt.plot(val_accuracies_naive_bayes, label='Naive Bayes', marker='d', color=threshold_color)\n",
    "\n",
    "# Plot means with low alpha, matching colors\n",
    "plt.plot(mean_cnn_time, label='Mean CNN Time', linestyle='--', alpha=0.3, color=cnn_time_color)\n",
    "plt.plot(mean_cnn_freq, label='Mean CNN Freq', linestyle='--', alpha=0.3, color=cnn_freq_color)\n",
    "plt.plot(mean_fnn_freq, label='Mean FNN Freq', linestyle='--', alpha=0.3, color=fnn_freq_color)\n",
    "plt.plot(mean_threshold, label='Mean Naive Bayes', linestyle='--', alpha=0.3, color=threshold_color)\n",
    "\n",
    "plt.title('Validation Accuracies with Means')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True)\n",
    "plt.ylim((0, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "19205c9ad57ecba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. How did it go?**"
   ],
   "id": "f0459050619dac94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
